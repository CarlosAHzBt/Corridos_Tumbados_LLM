{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook basada \n",
    "https://colab.research.google.com/drive/1mPw6P52cERr93w3CMBiJjocdTnyPiKTX#scrollTo=OlYkHgvXCWcb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Requisitos:\n",
    "Primero, asegúrate de tener las siguientes dependencias instaladas. Puedes usar pip para instalarlas.\n",
    "Estas bibliotecas son esenciales para trabajar con modelos grandes, hacer el fine-tuning con LoRA, y usar el soporte de 4 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.10.14)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pip install ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear este entorno virtual \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name unsloth_env \\\n",
    "    python=3.11 \\\n",
    "    pytorch-cuda=12.1 \\\n",
    "    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\n",
    "    -y\n",
    "conda activate unsloth_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to c:\\users\\carlo\\appdata\\local\\temp\\pip-install-t232x3ay\\unsloth_5e28d19ed51949ef8a09e63a0ac7046f\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 6c534341bb229b136f9504443f0161645d2070c5\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: packaging in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.1)\n",
      "Collecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached tyro-0.8.10-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting transformers>=4.43.2 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets>=2.16.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting tqdm (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.44.0)\n",
      "Collecting numpy (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached numpy-2.1.1-cp310-cp310-win_amd64.whl.metadata (59 kB)\n",
      "Collecting protobuf<4.0.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-win_amd64.whl.metadata (698 bytes)\n",
      "Collecting huggingface-hub (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hf-transfer (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading hf_transfer-0.1.8-cp310-none-win_amd64.whl.metadata (1.8 kB)\n",
      "Collecting filelock (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached filelock-3.16.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached pyarrow-17.0.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached pandas-2.2.2-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.32.2 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached aiohttp-3.10.5-cp310-cp310-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting pyyaml>=5.1 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from huggingface-hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from tqdm->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.6)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached regex-2024.9.11-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached safetensors-0.4.5-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting docstring-parser>=0.16 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached rich-13.8.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached multidict-6.1.0-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached yarl-1.11.1-cp310-cp310-win_amd64.whl.metadata (49 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Using cached datasets-3.0.0-py3-none-any.whl (474 kB)\n",
      "Using cached huggingface_hub-0.24.7-py3-none-any.whl (417 kB)\n",
      "Using cached numpy-2.1.1-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Downloading protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
      "   ---------------------------------------- 0.0/904.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 904.0/904.0 kB 10.4 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 991.5/991.5 kB 15.9 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Downloading hf_transfer-0.1.8-cp310-none-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 30.6 MB/s eta 0:00:00\n",
      "Using cached tyro-0.8.10-py3-none-any.whl (105 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached aiohttp-3.10.5-cp310-cp310-win_amd64.whl (379 kB)\n",
      "Using cached pyarrow-17.0.0-cp310-cp310-win_amd64.whl (25.1 MB)\n",
      "Using cached PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Using cached regex-2024.9.11-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached rich-13.8.1-py3-none-any.whl (241 kB)\n",
      "Using cached safetensors-0.4.5-cp310-none-win_amd64.whl (285 kB)\n",
      "Using cached shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "Using cached filelock-3.16.0-py3-none-any.whl (16 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached pandas-2.2.2-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Using cached xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached yarl-1.11.1-cp310-cp310-win_amd64.whl (110 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml): started\n",
      "  Building wheel for unsloth (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for unsloth: filename=unsloth-2024.8-py3-none-any.whl size=153935 sha256=97880e5685d6f7070c23cb411883ff73a014a4ca00bd64a69fcb7a73c959d80c\n",
      "  Stored in directory: C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-7hpzxtj8\\wheels\\ed\\d4\\e9\\76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
      "Successfully built unsloth\n",
      "Installing collected packages: sentencepiece, pytz, xxhash, urllib3, unsloth, tzdata, tqdm, shtab, safetensors, regex, pyyaml, protobuf, numpy, multidict, mdurl, idna, hf-transfer, fsspec, frozenlist, filelock, docstring-parser, dill, charset-normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, yarl, requests, pyarrow, pandas, multiprocess, markdown-it-py, aiosignal, rich, huggingface-hub, aiohttp, tyro, tokenizers, transformers, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 async-timeout-4.0.3 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.3.2 datasets-3.0.0 dill-0.3.8 docstring-parser-0.16 filelock-3.16.0 frozenlist-1.4.1 fsspec-2024.6.1 hf-transfer-0.1.8 huggingface-hub-0.24.7 idna-3.10 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 numpy-2.1.1 pandas-2.2.2 protobuf-3.20.3 pyarrow-17.0.0 pytz-2024.2 pyyaml-6.0.2 regex-2024.9.11 requests-2.32.3 rich-13.8.1 safetensors-0.4.5 sentencepiece-0.2.0 shtab-1.7.1 tokenizers-0.19.1 tqdm-4.66.5 transformers-4.44.2 tyro-0.8.10 tzdata-2024.1 unsloth-2024.8 urllib3-2.2.3 xxhash-3.5.0 yarl-1.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git 'C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-t232x3ay\\unsloth_5e28d19ed51949ef8a09e63a0ac7046f'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\n",
      "  Using cached trl-0.10.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.43.3-py3-none-win_amd64.whl.metadata (3.5 kB)\n",
      "Using cached trl-0.10.1-py3-none-any.whl (280 kB)\n",
      "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "Using cached accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "Using cached bitsandbytes-0.43.3-py3-none-win_amd64.whl (136.5 MB)\n",
      "Installing collected packages: bitsandbytes, trl, peft, accelerate\n",
      "Successfully installed accelerate-0.34.2 bitsandbytes-0.43.3 peft-0.12.0 trl-0.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "%pip install --no-deps trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from torchvision) (2.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar las bibliotecas necesarias\n",
    "asi como los recursos para unsloth\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page here. https://github.com/unslothai/unsloth#installation-instructions---conda\n",
    "\n",
    "You will learn how to do data prep, how to train, how to run the model, & how to save it (eg for Llama.cpp).\n",
    "\n",
    "[NEW] Llama-3 8b is trained on a crazy 15 trillion tokens! Llama-2 was 2 trillion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu118\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: packaging in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (24.1)\n",
      "Collecting ninja\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting einops\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "     --------------------------- ------------ 1.8/2.6 MB 14.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 15.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.28.post1.tar.gz (7.8 MB)\n",
      "     ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "     ---------------------------- ----------- 5.5/7.8 MB 28.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.8/7.8 MB 26.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: trl in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: peft in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (0.34.2)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\carlo\\documents\\codigo\\scrapping corridos tumbdos\\.conda\\lib\\site-packages (0.43.3)\n",
      "Using cached ninja-1.11.1.1-py2.py3-none-win_amd64.whl (312 kB)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Building wheels for collected packages: flash-attn, xformers\n",
      "  Building wheel for flash-attn (setup.py): started\n",
      "  Building wheel for flash-attn (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for flash-attn\n",
      "  Building wheel for xformers (setup.py): started\n",
      "  Building wheel for xformers (setup.py): still running...\n",
      "  Building wheel for xformers (setup.py): still running...\n",
      "  Building wheel for xformers (setup.py): still running...\n",
      "  Building wheel for xformers (setup.py): still running...\n",
      "  Building wheel for xformers (setup.py): still running...\n",
      "  Building wheel for xformers (setup.py): still running...\n",
      "  Building wheel for xformers (setup.py): still running...\n",
      "  Building wheel for xformers (setup.py): still running...\n",
      "  Building wheel for xformers (setup.py): still running...\n",
      "  Building wheel for xformers (setup.py): finished with status 'done'\n",
      "  Created wheel for xformers: filename=xformers-0.0.28.post1-cp310-cp310-win_amd64.whl size=4942555 sha256=2d48b200f909d74c4bda3a5ab791d7aa56d291c8ca943f58bcf3a830f84222ef\n",
      "  Stored in directory: c:\\users\\carlo\\appdata\\local\\pip\\cache\\wheels\\50\\c1\\5f\\298c4823de5030304b426a934752a0670ff58965cbdb9c5a8f\n",
      "Successfully built xformers\n",
      "Failed to build flash-attn\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [1789 lines of output]\n",
      "      fatal: not a git repository (or any of the parent directories): .git\n",
      "      \n",
      "      \n",
      "      torch.__version__  = 2.4.1+cu118\n",
      "      \n",
      "      \n",
      "      c:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\lib\\site-packages\\setuptools\\__init__.py:85: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Requirements should be satisfied by a PEP 517 installer.\n",
      "              If you are using pip, you can try `pip install --use-pep517`.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        dist.fetch_build_eggs(dist.setup_requires)\n",
      "      running bdist_wheel\n",
      "      Guessing wheel URL:  https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3+cu118torch2.4cxx11abiFALSE-cp310-cp310-win_amd64.whl\n",
      "      Precompiled wheel not found. Building from source...\n",
      "      c:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\lib\\site-packages\\torch\\utils\\cpp_extension.py:495: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "        warnings.warn(msg.format('we could not find ninja.'))\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-310\n",
      "      creating build\\lib.win-amd64-cpython-310\\flash_attn\n",
      "      copying flash_attn\\bert_padding.py -> build\\lib.win-amd64-cpython-310\\flash_attn\n",
      "      copying flash_attn\\flash_attn_interface.py -> build\\lib.win-amd64-cpython-310\\flash_attn\n",
      "      copying flash_attn\\flash_attn_triton.py -> build\\lib.win-amd64-cpython-310\\flash_attn\n",
      "      copying flash_attn\\flash_attn_triton_og.py -> build\\lib.win-amd64-cpython-310\\flash_attn\n",
      "      copying flash_attn\\flash_blocksparse_attention.py -> build\\lib.win-amd64-cpython-310\\flash_attn\n",
      "      copying flash_attn\\flash_blocksparse_attn_interface.py -> build\\lib.win-amd64-cpython-310\\flash_attn\n",
      "      copying flash_attn\\fused_softmax.py -> build\\lib.win-amd64-cpython-310\\flash_attn\n",
      "      copying flash_attn\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\n",
      "      creating build\\lib.win-amd64-cpython-310\\hopper\n",
      "      copying hopper\\benchmark_attn.py -> build\\lib.win-amd64-cpython-310\\hopper\n",
      "      copying hopper\\flash_attn_interface.py -> build\\lib.win-amd64-cpython-310\\hopper\n",
      "      copying hopper\\setup.py -> build\\lib.win-amd64-cpython-310\\hopper\n",
      "      copying hopper\\test_flash_attn.py -> build\\lib.win-amd64-cpython-310\\hopper\n",
      "      copying hopper\\__init__.py -> build\\lib.win-amd64-cpython-310\\hopper\n",
      "      creating build\\lib.win-amd64-cpython-310\\flash_attn\\layers\n",
      "      copying flash_attn\\layers\\patch_embed.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\layers\n",
      "      copying flash_attn\\layers\\rotary.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\layers\n",
      "      copying flash_attn\\layers\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\layers\n",
      "      creating build\\lib.win-amd64-cpython-310\\flash_attn\\losses\n",
      "      copying flash_attn\\losses\\cross_entropy.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\losses\n",
      "      copying flash_attn\\losses\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\losses\n",
      "      creating build\\lib.win-amd64-cpython-310\\flash_attn\\models\n",
      "      copying flash_attn\\models\\baichuan.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\n",
      "      copying flash_attn\\models\\bert.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\n",
      "      copying flash_attn\\models\\bigcode.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\n",
      "      copying flash_attn\\models\\btlm.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\n",
      "      copying flash_attn\\models\\falcon.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\n",
      "      copying flash_attn\\models\\gpt.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\n",
      "      copying flash_attn\\models\\gptj.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\n",
      "      copying flash_attn\\models\\gpt_neox.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\n",
      "      copying flash_attn\\models\\llama.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\n",
      "      copying flash_attn\\models\\opt.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\n",
      "      copying flash_attn\\models\\vit.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\n",
      "      copying flash_attn\\models\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\models\n",
      "      creating build\\lib.win-amd64-cpython-310\\flash_attn\\modules\n",
      "      copying flash_attn\\modules\\block.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\modules\n",
      "      copying flash_attn\\modules\\embedding.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\modules\n",
      "      copying flash_attn\\modules\\mha.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\modules\n",
      "      copying flash_attn\\modules\\mlp.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\modules\n",
      "      copying flash_attn\\modules\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\modules\n",
      "      creating build\\lib.win-amd64-cpython-310\\flash_attn\\ops\n",
      "      copying flash_attn\\ops\\activations.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\n",
      "      copying flash_attn\\ops\\fused_dense.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\n",
      "      copying flash_attn\\ops\\layer_norm.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\n",
      "      copying flash_attn\\ops\\rms_norm.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\n",
      "      copying flash_attn\\ops\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\n",
      "      creating build\\lib.win-amd64-cpython-310\\flash_attn\\utils\n",
      "      copying flash_attn\\utils\\benchmark.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\utils\n",
      "      copying flash_attn\\utils\\distributed.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\utils\n",
      "      copying flash_attn\\utils\\generation.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\utils\n",
      "      copying flash_attn\\utils\\pretrained.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\utils\n",
      "      copying flash_attn\\utils\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\utils\n",
      "      creating build\\lib.win-amd64-cpython-310\\flash_attn\\ops\\triton\n",
      "      copying flash_attn\\ops\\triton\\cross_entropy.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\\triton\n",
      "      copying flash_attn\\ops\\triton\\k_activations.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\\triton\n",
      "      copying flash_attn\\ops\\triton\\layer_norm.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\\triton\n",
      "      copying flash_attn\\ops\\triton\\linear.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\\triton\n",
      "      copying flash_attn\\ops\\triton\\mlp.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\\triton\n",
      "      copying flash_attn\\ops\\triton\\rotary.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\\triton\n",
      "      copying flash_attn\\ops\\triton\\__init__.py -> build\\lib.win-amd64-cpython-310\\flash_attn\\ops\\triton\n",
      "      running build_ext\n",
      "      c:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\lib\\site-packages\\torch\\utils\\cpp_extension.py:380: UserWarning: Error checking compiler version for cl: [WinError 2] El sistema no puede encontrar el archivo especificado\n",
      "        warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "      building 'flash_attn_2_cuda' extension\n",
      "      creating build\\temp.win-amd64-cpython-310\n",
      "      creating build\\temp.win-amd64-cpython-310\\Release\n",
      "      creating build\\temp.win-amd64-cpython-310\\Release\\csrc\n",
      "      creating build\\temp.win-amd64-cpython-310\\Release\\csrc\\flash_attn\n",
      "      creating build\\temp.win-amd64-cpython-310\\Release\\csrc\\flash_attn\\src\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn -IC:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src -IC:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\cutlass\\include \"-Ic:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\lib\\site-packages\\torch\\include\" \"-Ic:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\lib\\site-packages\\torch\\include\\torch\\csrc\\api\\include\" \"-Ic:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\lib\\site-packages\\torch\\include\\TH\" \"-Ic:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\lib\\site-packages\\torch\\include\\THC\" \"-IC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\include\" \"-Ic:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\include\" \"-Ic:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\Include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\" /EHsc /Tpcsrc/flash_attn/flash_api.cpp /Fobuild\\temp.win-amd64-cpython-310\\Release\\csrc/flash_attn/flash_api.obj /MD /wd4819 /wd4251 /wd4244 /wd4267 /wd4275 /wd4018 /wd4190 /wd4624 /wd4067 /wd4068 /EHsc -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0 /std:c++17\n",
      "      cl : L\\xa1nea de comandos warning D9002 : se omite la opci\\xa2n desconocida '-O3'\n",
      "      cl : L\\xa1nea de comandos warning D9002 : se omite la opci\\xa2n desconocida '-std=c++17'\n",
      "      flash_api.cpp\n",
      "      \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin\\nvcc\" -c csrc/flash_attn/src/flash_bwd_hdim128_bf16_causal_sm80.cu -o build\\temp.win-amd64-cpython-310\\Release\\csrc/flash_attn/src/flash_bwd_hdim128_bf16_causal_sm80.obj -IC:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn -IC:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src -IC:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\cutlass\\include \"-Ic:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\lib\\site-packages\\torch\\include\" \"-Ic:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\lib\\site-packages\\torch\\include\\torch\\csrc\\api\\include\" \"-Ic:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\lib\\site-packages\\torch\\include\\TH\" \"-Ic:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\lib\\site-packages\\torch\\include\\THC\" \"-IC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\include\" \"-Ic:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\include\" \"-Ic:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\Include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\" -Xcudafe --diag_suppress=dll_interface_conflict_dllexport_assumed -Xcudafe --diag_suppress=dll_interface_conflict_none_assumed -Xcudafe --diag_suppress=field_without_dll_interface -Xcudafe --diag_suppress=base_class_has_different_dll_interface -Xcompiler /EHsc -Xcompiler /wd4068 -Xcompiler /wd4067 -Xcompiler /wd4624 -Xcompiler /wd4190 -Xcompiler /wd4018 -Xcompiler /wd4275 -Xcompiler /wd4267 -Xcompiler /wd4244 -Xcompiler /wd4251 -Xcompiler /wd4819 -Xcompiler /MD -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17 --use-local-env\n",
      "      flash_bwd_hdim128_bf16_causal_sm80.cu\n",
      "      cl : L\\xa1nea de comandos warning D9025 : invalidando '/D__CUDA_NO_HALF_OPERATORS__' con '/U__CUDA_NO_HALF_OPERATORS__'\n",
      "      cl : L\\xa1nea de comandos warning D9025 : invalidando '/D__CUDA_NO_HALF_CONVERSIONS__' con '/U__CUDA_NO_HALF_CONVERSIONS__'\n",
      "      cl : L\\xa1nea de comandos warning D9025 : invalidando '/D__CUDA_NO_HALF2_OPERATORS__' con '/U__CUDA_NO_HALF2_OPERATORS__'\n",
      "      cl : L\\xa1nea de comandos warning D9025 : invalidando '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' con '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'\n",
      "      flash_bwd_hdim128_bf16_causal_sm80.cu\n",
      "      cl : L\\xa1nea de comandos warning D9025 : invalidando '/D__CUDA_NO_HALF_OPERATORS__' con '/U__CUDA_NO_HALF_OPERATORS__'\n",
      "      cl : L\\xa1nea de comandos warning D9025 : invalidando '/D__CUDA_NO_HALF_CONVERSIONS__' con '/U__CUDA_NO_HALF_CONVERSIONS__'\n",
      "      cl : L\\xa1nea de comandos warning D9025 : invalidando '/D__CUDA_NO_HALF2_OPERATORS__' con '/U__CUDA_NO_HALF2_OPERATORS__'\n",
      "      cl : L\\xa1nea de comandos warning D9025 : invalidando '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' con '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'\n",
      "      flash_bwd_hdim128_bf16_causal_sm80.cu\n",
      "      cl : L\\xa1nea de comandos warning D9025 : invalidando '/D__CUDA_NO_HALF_OPERATORS__' con '/U__CUDA_NO_HALF_OPERATORS__'\n",
      "      cl : L\\xa1nea de comandos warning D9025 : invalidando '/D__CUDA_NO_HALF_CONVERSIONS__' con '/U__CUDA_NO_HALF_CONVERSIONS__'\n",
      "      cl : L\\xa1nea de comandos warning D9025 : invalidando '/D__CUDA_NO_HALF2_OPERATORS__' con '/U__CUDA_NO_HALF2_OPERATORS__'\n",
      "      cl : L\\xa1nea de comandos warning D9025 : invalidando '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' con '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'\n",
      "      flash_bwd_hdim128_bf16_causal_sm80.cu\n",
      "      C:/Program Files (x86)/Windows Kits/10/include/10.0.22621.0/ucrt\\fenv.h(113): warning #550-D: variable \"_Ans\" was set but never used\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/functional.hpp(105): error: no instance of overloaded function \"cute::abs\" matches the argument list\n",
      "                  argument types are: (const cute::_1)\n",
      "                detected during:\n",
      "                  instantiation of \"decltype(auto) cute::abs_fn::operator()(T &&) const [with T=const cute::_1 &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(265): here\n",
      "                  instantiation of \"auto cute::transform_leaf(const T &, F &&) [with T=cute::_1, F=cute::abs_fn &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  instantiation of function \"lambda [](const auto &)->auto [with <auto-1>=cute::_1]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(114): here\n",
      "                  instantiation of \"auto cute::detail::tapply(T &&, F &&, G &&, cute::seq<I...>) [with T=const cute::tuple<cute::_1, cute::_8> &, F=lambda [](const auto &)->auto &, G=lambda [](const auto &...)->auto, I=<0, 1>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(218): here\n",
      "                  instantiation of \"auto cute::transform(const T &, F &&) [with T=cute::tuple<cute::_1, cute::_8>, F=lambda [](const auto &)->auto]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  [ 6 instantiation contexts not shown ]\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::ComposedLayout<A, O, B> &, const Shape &, const ModeOrder &) [with A=cute::Swizzle<3, 3, 3>, O=cute::C<0>, B=cute::Layout<cute::tuple<cute::_8, cute::_64>, cute::tuple<cute::_64, cute::_1>>, Shape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(216): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/functional.hpp(105): error: no instance of overloaded function \"cute::abs\" matches the argument list\n",
      "                  argument types are: (const cute::_8)\n",
      "                detected during:\n",
      "                  instantiation of \"decltype(auto) cute::abs_fn::operator()(T &&) const [with T=const cute::_8 &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(265): here\n",
      "                  instantiation of \"auto cute::transform_leaf(const T &, F &&) [with T=cute::_8, F=cute::abs_fn &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  instantiation of function \"lambda [](const auto &)->auto [with <auto-1>=cute::_8]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(114): here\n",
      "                  instantiation of \"auto cute::detail::tapply(T &&, F &&, G &&, cute::seq<I...>) [with T=const cute::tuple<cute::_1, cute::_8> &, F=lambda [](const auto &)->auto &, G=lambda [](const auto &...)->auto, I=<0, 1>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(218): here\n",
      "                  instantiation of \"auto cute::transform(const T &, F &&) [with T=cute::tuple<cute::_1, cute::_8>, F=lambda [](const auto &)->auto]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  [ 6 instantiation contexts not shown ]\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::ComposedLayout<A, O, B> &, const Shape &, const ModeOrder &) [with A=cute::Swizzle<3, 3, 3>, O=cute::C<0>, B=cute::Layout<cute::tuple<cute::_8, cute::_64>, cute::tuple<cute::_64, cute::_1>>, Shape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(216): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1520): error: no instance of overloaded function \"cute::composition\" matches the argument list\n",
      "                  argument types are: (<error-type>, const cute::Layout<cute::tuple<cute::C<8>, cute::C<2>>, cute::tuple<cute::_1, cute::_8>>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::logical_product(const cute::Layout<LShape, LStride> &, const cute::Layout<TShape, TStride> &) [with LShape=cute::tuple<cute::_8, cute::_64>, LStride=cute::tuple<cute::_64, cute::_1>, TShape=cute::tuple<cute::C<8>, cute::C<2>>, TStride=cute::tuple<cute::_1, cute::_8>]\"\n",
      "      (1603): here\n",
      "                  instantiation of \"auto cute::blocked_product(const cute::Layout<TShape, TStride> &, const cute::Layout<UShape, UStride> &) [with TShape=cute::tuple<cute::_8, cute::_64>, TStride=cute::tuple<cute::_64, cute::_1>, UShape=cute::tuple<cute::C<8>, cute::C<2>>, UStride=cute::tuple<cute::_1, cute::_8>]\"\n",
      "      (1662): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::Layout<Shape, Stride> &, const TrgShape &, const ModeOrder &) [with Shape=cute::tuple<cute::_8, cute::_64>, Stride=cute::tuple<cute::_64, cute::_1>, TrgShape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout_composed.hpp(552): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::ComposedLayout<A, O, B> &, const Shape &, const ModeOrder &) [with A=cute::Swizzle<3, 3, 3>, O=cute::C<0>, B=cute::Layout<cute::tuple<cute::_8, cute::_64>, cute::tuple<cute::_64, cute::_1>>, Shape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(216): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1605): error: no instance of overloaded function \"cute::coalesce\" matches the argument list\n",
      "                  argument types are: (<error-type>, cute::tuple<cute::_1, cute::_1>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::blocked_product(const cute::Layout<TShape, TStride> &, const cute::Layout<UShape, UStride> &) [with TShape=cute::tuple<cute::_8, cute::_64>, TStride=cute::tuple<cute::_64, cute::_1>, UShape=cute::tuple<cute::C<8>, cute::C<2>>, UStride=cute::tuple<cute::_1, cute::_8>]\"\n",
      "      (1662): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::Layout<Shape, Stride> &, const TrgShape &, const ModeOrder &) [with Shape=cute::tuple<cute::_8, cute::_64>, Stride=cute::tuple<cute::_64, cute::_1>, TrgShape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout_composed.hpp(552): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::ComposedLayout<A, O, B> &, const Shape &, const ModeOrder &) [with A=cute::Swizzle<3, 3, 3>, O=cute::C<0>, B=cute::Layout<cute::tuple<cute::_8, cute::_64>, cute::tuple<cute::_64, cute::_1>>, Shape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(216): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1662): error: no instance of overloaded function \"cute::coalesce\" matches the argument list\n",
      "                  argument types are: (<error-type>, cute::tuple<cute::C<8>, cute::C<2>>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::Layout<Shape, Stride> &, const TrgShape &, const ModeOrder &) [with Shape=cute::tuple<cute::_8, cute::_64>, Stride=cute::tuple<cute::_64, cute::_1>, TrgShape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout_composed.hpp(552): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::ComposedLayout<A, O, B> &, const Shape &, const ModeOrder &) [with A=cute::Swizzle<3, 3, 3>, O=cute::C<0>, B=cute::Layout<cute::tuple<cute::_8, cute::_64>, cute::tuple<cute::_64, cute::_1>>, Shape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(216): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/functional.hpp(105): error: no instance of overloaded function \"cute::abs\" matches the argument list\n",
      "                  argument types are: (const cute::C<16>)\n",
      "                detected during:\n",
      "                  instantiation of \"decltype(auto) cute::abs_fn::operator()(T &&) const [with T=const cute::C<16> &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(265): here\n",
      "                  instantiation of \"auto cute::transform_leaf(const T &, F &&) [with T=cute::C<16>, F=cute::abs_fn &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  instantiation of function \"lambda [](const auto &)->auto [with <auto-1>=cute::C<16>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(114): here\n",
      "                  instantiation of \"auto cute::detail::tapply(T &&, F &&, G &&, cute::seq<I...>) [with T=const cute::tuple<cute::_1, cute::_16> &, F=lambda [](const auto &)->auto &, G=lambda [](const auto &...)->auto, I=<0, 1>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(218): here\n",
      "                  instantiation of \"auto cute::transform(const T &, F &&) [with T=cute::tuple<cute::_1, cute::_16>, F=lambda [](const auto &)->auto]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  [ 6 instantiation contexts not shown ]\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::ComposedLayout<A, O, B> &, const Shape &, const ModeOrder &) [with A=cute::Swizzle<3, 3, 3>, O=cute::C<0>, B=cute::Layout<cute::tuple<cute::_8, cute::_64>, cute::tuple<cute::_64, cute::_1>>, Shape=cute::tuple<cute::C<128>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(225): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1520): error: no instance of overloaded function \"cute::composition\" matches the argument list\n",
      "                  argument types are: (<error-type>, const cute::Layout<cute::tuple<cute::_16, cute::_2>, cute::tuple<cute::_1, cute::_16>>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::logical_product(const cute::Layout<LShape, LStride> &, const cute::Layout<TShape, TStride> &) [with LShape=cute::tuple<cute::_8, cute::_64>, LStride=cute::tuple<cute::_64, cute::_1>, TShape=cute::tuple<cute::_16, cute::_2>, TStride=cute::tuple<cute::_1, cute::_16>]\"\n",
      "      (1603): here\n",
      "                  instantiation of \"auto cute::blocked_product(const cute::Layout<TShape, TStride> &, const cute::Layout<UShape, UStride> &) [with TShape=cute::tuple<cute::_8, cute::_64>, TStride=cute::tuple<cute::_64, cute::_1>, UShape=cute::tuple<cute::_16, cute::_2>, UStride=cute::tuple<cute::_1, cute::_16>]\"\n",
      "      (1662): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::Layout<Shape, Stride> &, const TrgShape &, const ModeOrder &) [with Shape=cute::tuple<cute::_8, cute::_64>, Stride=cute::tuple<cute::_64, cute::_1>, TrgShape=cute::tuple<cute::C<128>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout_composed.hpp(552): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::ComposedLayout<A, O, B> &, const Shape &, const ModeOrder &) [with A=cute::Swizzle<3, 3, 3>, O=cute::C<0>, B=cute::Layout<cute::tuple<cute::_8, cute::_64>, cute::tuple<cute::_64, cute::_1>>, Shape=cute::tuple<cute::C<128>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(225): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1605): error: no instance of overloaded function \"cute::coalesce\" matches the argument list\n",
      "                  argument types are: (<error-type>, cute::tuple<cute::_1, cute::_1>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::blocked_product(const cute::Layout<TShape, TStride> &, const cute::Layout<UShape, UStride> &) [with TShape=cute::tuple<cute::_8, cute::_64>, TStride=cute::tuple<cute::_64, cute::_1>, UShape=cute::tuple<cute::_16, cute::_2>, UStride=cute::tuple<cute::_1, cute::_16>]\"\n",
      "      (1662): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::Layout<Shape, Stride> &, const TrgShape &, const ModeOrder &) [with Shape=cute::tuple<cute::_8, cute::_64>, Stride=cute::tuple<cute::_64, cute::_1>, TrgShape=cute::tuple<cute::C<128>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout_composed.hpp(552): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::ComposedLayout<A, O, B> &, const Shape &, const ModeOrder &) [with A=cute::Swizzle<3, 3, 3>, O=cute::C<0>, B=cute::Layout<cute::tuple<cute::_8, cute::_64>, cute::tuple<cute::_64, cute::_1>>, Shape=cute::tuple<cute::C<128>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(225): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1662): error: no instance of overloaded function \"cute::coalesce\" matches the argument list\n",
      "                  argument types are: (<error-type>, cute::tuple<cute::_16, cute::_2>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::Layout<Shape, Stride> &, const TrgShape &, const ModeOrder &) [with Shape=cute::tuple<cute::_8, cute::_64>, Stride=cute::tuple<cute::_64, cute::_1>, TrgShape=cute::tuple<cute::C<128>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout_composed.hpp(552): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::ComposedLayout<A, O, B> &, const Shape &, const ModeOrder &) [with A=cute::Swizzle<3, 3, 3>, O=cute::C<0>, B=cute::Layout<cute::tuple<cute::_8, cute::_64>, cute::tuple<cute::_64, cute::_1>>, Shape=cute::tuple<cute::C<128>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(225): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(228): error: no instance of overloaded function \"composition\" matches the argument list\n",
      "                  argument types are: (<error-type>, cute::Layout<cute::tuple<cute::C<128>, cute::C<128>>, cute::tuple<cute::_128, cute::_1>>)\n",
      "                detected during:\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(229): error: identifier \"get_nonswizzle_portion\" is undefined\n",
      "                detected during:\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/functional.hpp(105): error: no instance of overloaded function \"cute::abs\" matches the argument list\n",
      "                  argument types are: (const cute::C<0>)\n",
      "                detected during:\n",
      "                  instantiation of \"decltype(auto) cute::abs_fn::operator()(T &&) const [with T=const cute::C<0> &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(265): here\n",
      "                  instantiation of \"auto cute::transform_leaf(const T &, F &&) [with T=cute::C<0>, F=cute::abs_fn &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  instantiation of function \"lambda [](const auto &)->auto [with <auto-1>=cute::C<0>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(114): here\n",
      "                  instantiation of \"auto cute::detail::tapply(T &&, F &&, G &&, cute::seq<I...>) [with T=const cute::tuple<cute::C<0>, cute::C<1>> &, F=lambda [](const auto &)->auto &, G=lambda [](const auto &...)->auto, I=<0, 1>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(218): here\n",
      "                  instantiation of \"auto cute::transform(const T &, F &&) [with T=cute::tuple<cute::C<0>, cute::C<1>>, F=lambda [](const auto &)->auto]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  [ 6 instantiation contexts not shown ]\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::ComposedLayout<A, O, B> &, const Shape &, const ModeOrder &) [with A=cute::Swizzle<3, 3, 3>, O=cute::C<0>, B=cute::Layout<cute::tuple<cute::C<64>, cute::C<64>>, cute::tuple<cute::_64, cute::_1>>, Shape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(248): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1520): error: no instance of overloaded function \"cute::composition\" matches the argument list\n",
      "                  argument types are: (<error-type>, const cute::Layout<cute::tuple<cute::_1, cute::_2>, cute::tuple<cute::C<0>, cute::C<1>>>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::logical_product(const cute::Layout<LShape, LStride> &, const cute::Layout<TShape, TStride> &) [with LShape=cute::tuple<cute::C<64>, cute::C<64>>, LStride=cute::tuple<cute::_64, cute::_1>, TShape=cute::tuple<cute::_1, cute::_2>, TStride=cute::tuple<cute::C<0>, cute::C<1>>]\"\n",
      "      (1603): here\n",
      "                  instantiation of \"auto cute::blocked_product(const cute::Layout<TShape, TStride> &, const cute::Layout<UShape, UStride> &) [with TShape=cute::tuple<cute::C<64>, cute::C<64>>, TStride=cute::tuple<cute::_64, cute::_1>, UShape=cute::tuple<cute::_1, cute::_2>, UStride=cute::tuple<cute::C<0>, cute::C<1>>]\"\n",
      "      (1662): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::Layout<Shape, Stride> &, const TrgShape &, const ModeOrder &) [with Shape=cute::tuple<cute::C<64>, cute::C<64>>, Stride=cute::tuple<cute::_64, cute::_1>, TrgShape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout_composed.hpp(552): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::ComposedLayout<A, O, B> &, const Shape &, const ModeOrder &) [with A=cute::Swizzle<3, 3, 3>, O=cute::C<0>, B=cute::Layout<cute::tuple<cute::C<64>, cute::C<64>>, cute::tuple<cute::_64, cute::_1>>, Shape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(248): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1605): error: no instance of overloaded function \"cute::coalesce\" matches the argument list\n",
      "                  argument types are: (<error-type>, cute::tuple<cute::_1, cute::_1>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::blocked_product(const cute::Layout<TShape, TStride> &, const cute::Layout<UShape, UStride> &) [with TShape=cute::tuple<cute::C<64>, cute::C<64>>, TStride=cute::tuple<cute::_64, cute::_1>, UShape=cute::tuple<cute::_1, cute::_2>, UStride=cute::tuple<cute::C<0>, cute::C<1>>]\"\n",
      "      (1662): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::Layout<Shape, Stride> &, const TrgShape &, const ModeOrder &) [with Shape=cute::tuple<cute::C<64>, cute::C<64>>, Stride=cute::tuple<cute::_64, cute::_1>, TrgShape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout_composed.hpp(552): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::ComposedLayout<A, O, B> &, const Shape &, const ModeOrder &) [with A=cute::Swizzle<3, 3, 3>, O=cute::C<0>, B=cute::Layout<cute::tuple<cute::C<64>, cute::C<64>>, cute::tuple<cute::_64, cute::_1>>, Shape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(248): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1662): error: no instance of overloaded function \"cute::coalesce\" matches the argument list\n",
      "                  argument types are: (<error-type>, cute::tuple<cute::_1, cute::_2>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::Layout<Shape, Stride> &, const TrgShape &, const ModeOrder &) [with Shape=cute::tuple<cute::C<64>, cute::C<64>>, Stride=cute::tuple<cute::_64, cute::_1>, TrgShape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout_composed.hpp(552): here\n",
      "                  instantiation of \"auto cute::tile_to_shape(const cute::ComposedLayout<A, O, B> &, const Shape &, const ModeOrder &) [with A=cute::Swizzle<3, 3, 3>, O=cute::C<0>, B=cute::Layout<cute::tuple<cute::C<64>, cute::C<64>>, cute::tuple<cute::_64, cute::_1>>, Shape=cute::tuple<cute::C<64>, cute::C<128>>, ModeOrder=cute::GenColMajor]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(248): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(250): error: no instance of overloaded function \"composition\" matches the argument list\n",
      "                  argument types are: (<error-type>, cute::Layout<cute::tuple<cute::C<128>, cute::C<64>>, cute::tuple<cute::_64, cute::_1>>)\n",
      "                detected during:\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(256): error: no instance of overloaded function \"composition\" matches the argument list\n",
      "                  argument types are: (<error-type>, cute::Layout<cute::tuple<cute::C<128>, cute::C<64>>, cute::tuple<cute::_64, cute::_1>>)\n",
      "                detected during:\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(286): error: no instance of overloaded function \"std::max\" matches the argument list\n",
      "                  argument types are: (const int, <error-type>)\n",
      "                detected during:\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(290): error: no instance of overloaded function \"std::max\" matches the argument list\n",
      "                  argument types are: (const int, <error-type>)\n",
      "                detected during:\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1680): error: no instance of overloaded function \"cute::abs\" matches the argument list\n",
      "                  argument types are: (const cute::C<1>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::upcast<N,Shape,Stride>(const Shape &, const Stride &) [with N=16, Shape=cute::_128, Stride=cute::C<1>]\"\n",
      "      (1676): here\n",
      "                  instantiation of function \"lambda [](const auto &, const auto &)->auto [with <auto-1>=cute::_128, <auto-2>=cute::C<1>]\"\n",
      "      (704): here\n",
      "                  instantiation of \"auto cute::detail::transform_layout(const Tuple0 &, const Tuple1 &, F &&, cute::seq<I...>, cute::seq<I0...>, cute::seq<I1...>) [with Tuple0=cute::tuple<cute::_1, cute::_128>, Tuple1=cute::tuple<cute::C<0>, cute::C<1>>, F=lambda [](const auto &, const auto &)->auto &, I=<0, 1>, I0=<>, I1=<>]\"\n",
      "      (725): here\n",
      "                  instantiation of \"auto cute::transform_layout(const Tuple0 &, const Tuple1 &, F &&) [with Tuple0=cute::tuple<cute::_1, cute::_128>, Tuple1=cute::tuple<cute::C<0>, cute::C<1>>, F=lambda [](const auto &, const auto &)->auto]\"\n",
      "      (1676): here\n",
      "                  instantiation of \"auto cute::upcast<N,Shape,Stride>(const Shape &, const Stride &) [with N=16, Shape=cute::tuple<cute::_1, cute::_128>, Stride=cute::tuple<cute::C<0>, cute::C<1>>]\"\n",
      "      (1696): here\n",
      "                  [ 3 instantiation contexts not shown ]\n",
      "                  instantiation of class \"cute::Copy_Atom<CopyOperation, CopyInternalType> [with CopyOperation=cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, CopyInternalType=cutlass::bfloat16_t]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(309): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1680): error: no instance of overloaded function \"cute::make_layout\" matches the argument list\n",
      "                  argument types are: (<error-type>, cute::C<1>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::upcast<N,Shape,Stride>(const Shape &, const Stride &) [with N=16, Shape=cute::_128, Stride=cute::C<1>]\"\n",
      "      (1676): here\n",
      "                  instantiation of function \"lambda [](const auto &, const auto &)->auto [with <auto-1>=cute::_128, <auto-2>=cute::C<1>]\"\n",
      "      (704): here\n",
      "                  instantiation of \"auto cute::detail::transform_layout(const Tuple0 &, const Tuple1 &, F &&, cute::seq<I...>, cute::seq<I0...>, cute::seq<I1...>) [with Tuple0=cute::tuple<cute::_1, cute::_128>, Tuple1=cute::tuple<cute::C<0>, cute::C<1>>, F=lambda [](const auto &, const auto &)->auto &, I=<0, 1>, I0=<>, I1=<>]\"\n",
      "      (725): here\n",
      "                  instantiation of \"auto cute::transform_layout(const Tuple0 &, const Tuple1 &, F &&) [with Tuple0=cute::tuple<cute::_1, cute::_128>, Tuple1=cute::tuple<cute::C<0>, cute::C<1>>, F=lambda [](const auto &, const auto &)->auto]\"\n",
      "      (1676): here\n",
      "                  instantiation of \"auto cute::upcast<N,Shape,Stride>(const Shape &, const Stride &) [with N=16, Shape=cute::tuple<cute::_1, cute::_128>, Stride=cute::tuple<cute::C<0>, cute::C<1>>]\"\n",
      "      (1696): here\n",
      "                  [ 3 instantiation contexts not shown ]\n",
      "                  instantiation of class \"cute::Copy_Atom<CopyOperation, CopyInternalType> [with CopyOperation=cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, CopyInternalType=cutlass::bfloat16_t]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(309): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1520): error: no instance of overloaded function \"cute::composition\" matches the argument list\n",
      "                  argument types are: (<error-type>, const cute::Layout<cute::tuple<cute::_1, cute::_8>, cute::tuple<cute::C<0>, cute::C<1>>>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::logical_product(const cute::Layout<LShape, LStride> &, const cute::Layout<TShape, TStride> &) [with LShape=cute::tuple<cute::C<32>, cute::C<8>>, LStride=cute::tuple<cute::_8, cute::_1>, TShape=cute::tuple<cute::_1, cute::_8>, TStride=cute::tuple<cute::C<0>, cute::C<1>>]\"\n",
      "      (1621): here\n",
      "                  instantiation of \"auto cute::raked_product(const cute::Layout<TShape, TStride> &, const cute::Layout<UShape, UStride> &) [with TShape=cute::tuple<cute::C<32>, cute::C<8>>, TStride=cute::tuple<cute::_8, cute::_1>, UShape=cute::tuple<cute::_1, cute::_8>, UStride=cute::tuple<cute::C<0>, cute::C<1>>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/atom/copy_atom.hpp(484): here\n",
      "                  instantiation of \"auto cute::make_tiled_copy(const cute::Copy_Atom<Args...> &, const ThrLayout &, const ValLayout &) [with Args=<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, cutlass::bfloat16_t>, ThrLayout=cute::Layout<cute::tuple<cute::C<32>, cute::C<8>>, cute::tuple<cute::_8, cute::_1>>, ValLayout=cute::Layout<cute::tuple<cute::_1, cute::_8>, cute::tuple<cute::C<0>, cute::C<1>>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(311): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1623): error: no instance of overloaded function \"cute::coalesce\" matches the argument list\n",
      "                  argument types are: (<error-type>, cute::tuple<cute::_1, cute::_1>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::raked_product(const cute::Layout<TShape, TStride> &, const cute::Layout<UShape, UStride> &) [with TShape=cute::tuple<cute::C<32>, cute::C<8>>, TStride=cute::tuple<cute::_8, cute::_1>, UShape=cute::tuple<cute::_1, cute::_8>, UStride=cute::tuple<cute::C<0>, cute::C<1>>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/atom/copy_atom.hpp(484): here\n",
      "                  instantiation of \"auto cute::make_tiled_copy(const cute::Copy_Atom<Args...> &, const ThrLayout &, const ValLayout &) [with Args=<cute::SM80_CP_ASYNC_CACHEGLOBAL<cutlass::uint128_t, cutlass::uint128_t>, cutlass::bfloat16_t>, ThrLayout=cute::Layout<cute::tuple<cute::C<32>, cute::C<8>>, cute::tuple<cute::_8, cute::_1>>, ValLayout=cute::Layout<cute::tuple<cute::_1, cute::_8>, cute::tuple<cute::C<0>, cute::C<1>>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(311): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1520): error: no instance of overloaded function \"cute::composition\" matches the argument list\n",
      "                  argument types are: (<error-type>, const cute::Layout<cute::tuple<cute::_1, cute::_4>, cute::tuple<cute::C<0>, cute::C<1>>>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::logical_product(const cute::Layout<LShape, LStride> &, const cute::Layout<TShape, TStride> &) [with LShape=cute::tuple<cute::C<16>, cute::C<16>>, LStride=cute::tuple<cute::_16, cute::_1>, TShape=cute::tuple<cute::_1, cute::_4>, TStride=cute::tuple<cute::C<0>, cute::C<1>>]\"\n",
      "      (1621): here\n",
      "                  instantiation of \"auto cute::raked_product(const cute::Layout<TShape, TStride> &, const cute::Layout<UShape, UStride> &) [with TShape=cute::tuple<cute::C<16>, cute::C<16>>, TStride=cute::tuple<cute::_16, cute::_1>, UShape=cute::tuple<cute::_1, cute::_4>, UStride=cute::tuple<cute::C<0>, cute::C<1>>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/atom/copy_atom.hpp(484): here\n",
      "                  instantiation of \"auto cute::make_tiled_copy(const cute::Copy_Atom<Args...> &, const ThrLayout &, const ValLayout &) [with Args=<cute::DefaultCopy, float>, ThrLayout=cute::Layout<cute::tuple<cute::C<16>, cute::C<16>>, cute::tuple<cute::_16, cute::_1>>, ValLayout=cute::Layout<cute::tuple<cute::_1, cute::_4>, cute::tuple<cute::C<0>, cute::C<1>>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(334): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1623): error: no instance of overloaded function \"cute::coalesce\" matches the argument list\n",
      "                  argument types are: (<error-type>, cute::tuple<cute::_1, cute::_1>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::raked_product(const cute::Layout<TShape, TStride> &, const cute::Layout<UShape, UStride> &) [with TShape=cute::tuple<cute::C<16>, cute::C<16>>, TStride=cute::tuple<cute::_16, cute::_1>, UShape=cute::tuple<cute::_1, cute::_4>, UStride=cute::tuple<cute::C<0>, cute::C<1>>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/atom/copy_atom.hpp(484): here\n",
      "                  instantiation of \"auto cute::make_tiled_copy(const cute::Copy_Atom<Args...> &, const ThrLayout &, const ValLayout &) [with Args=<cute::DefaultCopy, float>, ThrLayout=cute::Layout<cute::tuple<cute::C<16>, cute::C<16>>, cute::tuple<cute::_16, cute::_1>>, ValLayout=cute::Layout<cute::tuple<cute::_1, cute::_4>, cute::tuple<cute::C<0>, cute::C<1>>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(334): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1520): error: no instance of overloaded function \"cute::composition\" matches the argument list\n",
      "                  argument types are: (<error-type>, const cute::Layout<cute::tuple<cute::_1, cute::_1>, cute::tuple<cute::_0, cute::_0>>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::logical_product(const cute::Layout<LShape, LStride> &, const cute::Layout<TShape, TStride> &) [with LShape=cute::tuple<cute::C<8>, cute::C<32>>, LStride=cute::tuple<cute::_32, cute::_1>, TShape=cute::tuple<cute::_1, cute::_1>, TStride=cute::tuple<cute::_0, cute::_0>]\"\n",
      "      (1621): here\n",
      "                  instantiation of \"auto cute::raked_product(const cute::Layout<TShape, TStride> &, const cute::Layout<UShape, UStride> &) [with TShape=cute::tuple<cute::C<8>, cute::C<32>>, TStride=cute::tuple<cute::_32, cute::_1>, UShape=cute::tuple<cute::_1, cute::_1>, UStride=cute::tuple<cute::_0, cute::_0>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/atom/copy_atom.hpp(484): here\n",
      "                  instantiation of \"auto cute::make_tiled_copy(const cute::Copy_Atom<Args...> &, const ThrLayout &, const ValLayout &) [with Args=<cute::DefaultCopy, float>, ThrLayout=cute::Layout<cute::tuple<cute::C<8>, cute::C<32>>, cute::tuple<cute::_32, cute::_1>>, ValLayout=cute::Layout<cute::tuple<cute::_1, cute::_1>, cute::tuple<cute::_0, cute::_0>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(340): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1623): error: no instance of overloaded function \"cute::coalesce\" matches the argument list\n",
      "                  argument types are: (<error-type>, cute::tuple<cute::_1, cute::_1>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::raked_product(const cute::Layout<TShape, TStride> &, const cute::Layout<UShape, UStride> &) [with TShape=cute::tuple<cute::C<8>, cute::C<32>>, TStride=cute::tuple<cute::_32, cute::_1>, UShape=cute::tuple<cute::_1, cute::_1>, UStride=cute::tuple<cute::_0, cute::_0>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/atom/copy_atom.hpp(484): here\n",
      "                  instantiation of \"auto cute::make_tiled_copy(const cute::Copy_Atom<Args...> &, const ThrLayout &, const ValLayout &) [with Args=<cute::DefaultCopy, float>, ThrLayout=cute::Layout<cute::tuple<cute::C<8>, cute::C<32>>, cute::tuple<cute::_32, cute::_1>>, ValLayout=cute::Layout<cute::tuple<cute::_1, cute::_1>, cute::tuple<cute::_0, cute::_0>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\kernel_traits.h(340): here\n",
      "                  instantiation of class \"Flash_bwd_kernel_traits<kHeadDim_, kBlockM_, kBlockN_, kNWarps_, AtomLayoutMSdP_, AtomLayoutNdKV, AtomLayoutMdQ, Is_V_in_regs_, No_double_buffer_, elem_type, Base> [with kHeadDim_=128, kBlockM_=64, kBlockN_=128, kNWarps_=8, AtomLayoutMSdP_=2, AtomLayoutNdKV=4, AtomLayoutMdQ=2, Is_V_in_regs_=false, No_double_buffer_=false, elem_type=cutlass::bfloat16_t, Base=Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(70): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(104): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(105): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(106): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/numeric/arithmetic_tuple.hpp(407): error: no instance of overloaded function \"cute::abs\" matches the argument list\n",
      "                  argument types are: (cute::_1)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::abs(const cute::ScaledBasis<T, N> &) [with T=cute::C<1>, N=0]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/functional.hpp(105): here\n",
      "                  instantiation of \"decltype(auto) cute::abs_fn::operator()(T &&) const [with T=const cute::ScaledBasis<cute::C<1>, 0> &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(265): here\n",
      "                  instantiation of \"auto cute::transform_leaf(const T &, F &&) [with T=cute::ScaledBasis<cute::C<1>, 0>, F=cute::abs_fn &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  instantiation of function \"lambda [](const auto &)->auto [with <auto-1>=cute::ScaledBasis<cute::C<1>, 0>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(114): here\n",
      "                  instantiation of \"auto cute::detail::tapply(T &&, F &&, G &&, cute::seq<I...>) [with T=const cute::tuple<cute::ScaledBasis<cute::C<1>, 0>, cute::ScaledBasis<cute::C<1>, 1>> &, F=lambda [](const auto &)->auto &, G=lambda [](const auto &...)->auto, I=<0, 1>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(218): here\n",
      "                  [ 5 instantiation contexts not shown ]\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/container/tuple.hpp(170): error: incomplete type is not allowed\n",
      "                detected during:\n",
      "                  instantiation of class \"cute::tuple<T...> [with T=<<error-type>>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/numeric/arithmetic_tuple.hpp(234): here\n",
      "                  instantiation of class \"cute::ScaledBasis<T, N> [with T=<error-type>, N=0]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/numeric/arithmetic_tuple.hpp(407): here\n",
      "                  instantiation of \"auto cute::abs(const cute::ScaledBasis<T, N> &) [with T=cute::C<1>, N=0]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/functional.hpp(105): here\n",
      "                  instantiation of \"decltype(auto) cute::abs_fn::operator()(T &&) const [with T=const cute::ScaledBasis<cute::C<1>, 0> &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(265): here\n",
      "                  instantiation of \"auto cute::transform_leaf(const T &, F &&) [with T=cute::ScaledBasis<cute::C<1>, 0>, F=cute::abs_fn &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  [ 7 instantiation contexts not shown ]\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/numeric/arithmetic_tuple.hpp(407): error: no instance of overloaded function \"cute::abs\" matches the argument list\n",
      "                  argument types are: (cute::_1)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::abs(const cute::ScaledBasis<T, N> &) [with T=cute::C<1>, N=0]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/functional.hpp(105): here\n",
      "                  instantiation of \"decltype(auto) cute::abs_fn::operator()(T &&) const [with T=const cute::ScaledBasis<cute::C<1>, 0> &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(265): here\n",
      "                  instantiation of \"auto cute::transform_leaf(const T &, F &&) [with T=cute::ScaledBasis<cute::C<1>, 0>, F=cute::abs_fn &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  instantiation of function \"lambda [](const auto &)->auto [with <auto-1>=cute::ScaledBasis<cute::C<1>, 0>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(114): here\n",
      "                  instantiation of \"auto cute::detail::tapply(T &&, F &&, G &&, cute::seq<I...>) [with T=const cute::tuple<cute::ScaledBasis<cute::C<1>, 0>, cute::ScaledBasis<cute::C<1>, 1>> &, F=lambda [](const auto &)->auto &, G=lambda [](const auto &...)->auto, I=<0, 1>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(218): here\n",
      "                  [ 5 instantiation contexts not shown ]\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/numeric/arithmetic_tuple.hpp(407): error: no instance of overloaded function \"cute::abs\" matches the argument list\n",
      "                  argument types are: (cute::_1)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::abs(const cute::ScaledBasis<T, N> &) [with T=cute::C<1>, N=1]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/functional.hpp(105): here\n",
      "                  instantiation of \"decltype(auto) cute::abs_fn::operator()(T &&) const [with T=const cute::ScaledBasis<cute::C<1>, 1> &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(265): here\n",
      "                  instantiation of \"auto cute::transform_leaf(const T &, F &&) [with T=cute::ScaledBasis<cute::C<1>, 1>, F=cute::abs_fn &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  instantiation of function \"lambda [](const auto &)->auto [with <auto-1>=cute::ScaledBasis<cute::C<1>, 1>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(114): here\n",
      "                  instantiation of \"auto cute::detail::tapply(T &&, F &&, G &&, cute::seq<I...>) [with T=const cute::tuple<cute::ScaledBasis<cute::C<1>, 0>, cute::ScaledBasis<cute::C<1>, 1>> &, F=lambda [](const auto &)->auto &, G=lambda [](const auto &...)->auto, I=<0, 1>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(218): here\n",
      "                  [ 5 instantiation contexts not shown ]\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/numeric/arithmetic_tuple.hpp(407): error: no instance of overloaded function \"cute::abs\" matches the argument list\n",
      "                  argument types are: (cute::_1)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::abs(const cute::ScaledBasis<T, N> &) [with T=cute::C<1>, N=1]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/functional.hpp(105): here\n",
      "                  instantiation of \"decltype(auto) cute::abs_fn::operator()(T &&) const [with T=const cute::ScaledBasis<cute::C<1>, 1> &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(265): here\n",
      "                  instantiation of \"auto cute::transform_leaf(const T &, F &&) [with T=cute::ScaledBasis<cute::C<1>, 1>, F=cute::abs_fn &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  instantiation of function \"lambda [](const auto &)->auto [with <auto-1>=cute::ScaledBasis<cute::C<1>, 1>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(114): here\n",
      "                  instantiation of \"auto cute::detail::tapply(T &&, F &&, G &&, cute::seq<I...>) [with T=const cute::tuple<cute::ScaledBasis<cute::C<1>, 0>, cute::ScaledBasis<cute::C<1>, 1>> &, F=lambda [](const auto &)->auto &, G=lambda [](const auto &...)->auto, I=<0, 1>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(218): here\n",
      "                  [ 5 instantiation contexts not shown ]\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(108): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(109): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/tensor.hpp(332): error: static assertion failed with \"Dynamic owning tensors not supported\"\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::MakeTensor<T>::operator()(const Layout &) const [with T=__nv_bool, Layout=cute::Layout<__nv_bool, cute::_1>, <unnamed>=(void *)nullptr]\"\n",
      "      (352): here\n",
      "                  instantiation of \"auto cute::MakeTensor<T>::operator()(const LayoutArg &, const LayoutArgs &...) const [with T=__nv_bool, LayoutArg=__nv_bool, LayoutArgs=<>, <unnamed>=(void *)nullptr]\"\n",
      "      (385): here\n",
      "                  instantiation of \"auto cute::make_tensor(const Iterator &, const Args &...) [with Iterator=__nv_bool, Args=<>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(112): here\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(634): error: name followed by \"::\" must be a class or namespace name\n",
      "                detected during:\n",
      "                  instantiation of \"const int cute::cosize_v [with Layout=cute::Layout<__nv_bool, cute::_1>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/tensor.hpp(333): here\n",
      "                  instantiation of \"auto cute::MakeTensor<T>::operator()(const Layout &) const [with T=__nv_bool, Layout=cute::Layout<__nv_bool, cute::_1>, <unnamed>=(void *)nullptr]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/tensor.hpp(352): here\n",
      "                  instantiation of \"auto cute::MakeTensor<T>::operator()(const LayoutArg &, const LayoutArgs &...) const [with T=__nv_bool, LayoutArg=__nv_bool, LayoutArgs=<>, <unnamed>=(void *)nullptr]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/tensor.hpp(385): here\n",
      "                  instantiation of \"auto cute::make_tensor(const Iterator &, const Args &...) [with Iterator=__nv_bool, Args=<>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(112): here\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/container/tuple.hpp(170): error: incomplete type is not allowed\n",
      "                detected during:\n",
      "                  instantiation of class \"cute::tuple<T...> [with T=<cute::Layout<__nv_bool, cute::_1>, cute::ArrayEngine<__nv_bool, <error-constant>>>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/tensor.hpp(312): here\n",
      "                  instantiation of class \"cute::Tensor<Engine, Layout> [with Engine=cute::ArrayEngine<__nv_bool, <error-constant>>, Layout=cute::Layout<__nv_bool, cute::_1>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/tensor.hpp(334): here\n",
      "                  instantiation of \"auto cute::MakeTensor<T>::operator()(const Layout &) const [with T=__nv_bool, Layout=cute::Layout<__nv_bool, cute::_1>, <unnamed>=(void *)nullptr]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/tensor.hpp(352): here\n",
      "                  instantiation of \"auto cute::MakeTensor<T>::operator()(const LayoutArg &, const LayoutArgs &...) const [with T=__nv_bool, LayoutArg=__nv_bool, LayoutArgs=<>, <unnamed>=(void *)nullptr]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/tensor.hpp(385): here\n",
      "                  instantiation of \"auto cute::make_tensor(const Iterator &, const Args &...) [with Iterator=__nv_bool, Args=<>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(112): here\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(112): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(117): error: no instance of overloaded function \"make_fragment_like\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(117): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(118): error: no instance of overloaded function \"make_fragment_like\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(118): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(119): error: no instance of function template \"flash::copy\" matches the argument list\n",
      "                  argument types are: (<error-type>, <error-type>, <error-type>, <error-type>, <error-type>, int)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(122): error: no instance of function template \"flash::copy\" matches the argument list\n",
      "                  argument types are: (<error-type>, <error-type>, <error-type>, <error-type>, <error-type>, int)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(128): error: no instance of function template \"flash::dot_do_o\" matches the argument list\n",
      "                  argument types are: (<error-type>, <error-type>, cute::Tensor<cute::ViewEngine<cute::gmem_ptr<float *>>, cute::Layout<cute::tuple<cute::C<64>>, cute::tuple<cute::C<1>>>>, int, const float)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(133): error: no instance of overloaded function \"make_fragment_like\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(133): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(134): error: no instance of overloaded function \"clear\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(135): error: no instance of overloaded function \"cute::copy\" matches the argument list\n",
      "                  argument types are: (<error-type>, <error-type>, <error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=true, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(81): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(104): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(105): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(106): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(108): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(109): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(112): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(117): error: no instance of overloaded function \"make_fragment_like\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(117): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(118): error: no instance of overloaded function \"make_fragment_like\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(118): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(119): error: no instance of function template \"flash::copy\" matches the argument list\n",
      "                  argument types are: (<error-type>, <error-type>, <error-type>, <error-type>, <error-type>, int)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(122): error: no instance of function template \"flash::copy\" matches the argument list\n",
      "                  argument types are: (<error-type>, <error-type>, <error-type>, <error-type>, <error-type>, int)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(128): error: no instance of function template \"flash::dot_do_o\" matches the argument list\n",
      "                  argument types are: (<error-type>, <error-type>, cute::Tensor<cute::ViewEngine<cute::gmem_ptr<float *>>, cute::Layout<cute::tuple<cute::C<64>>, cute::tuple<cute::C<1>>>>, int, const float)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(133): error: no instance of overloaded function \"make_fragment_like\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(133): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(134): error: no instance of overloaded function \"clear\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_preprocess_kernel.h(135): error: no instance of overloaded function \"cute::copy\" matches the argument list\n",
      "                  argument types are: (<error-type>, <error-type>, <error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dot_do_o<Clear_dQaccum,Kernel_traits,Params>(const Params &) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(50): here\n",
      "                  instantiation of \"void flash_bwd_dot_do_o_kernel<Clear_dQaccum,Kernel_traits>(Flash_bwd_params) [with Clear_dQaccum=false, Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(83): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/functional.hpp(105): error: no instance of overloaded function \"cute::abs\" matches the argument list\n",
      "                  argument types are: (const cute::_2)\n",
      "                detected during:\n",
      "                  instantiation of \"decltype(auto) cute::abs_fn::operator()(T &&) const [with T=const cute::_2 &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(265): here\n",
      "                  instantiation of \"auto cute::transform_leaf(const T &, F &&) [with T=cute::_2, F=cute::abs_fn &]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  instantiation of function \"lambda [](const auto &)->auto [with <auto-1>=cute::_2]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(114): here\n",
      "                  instantiation of \"auto cute::detail::tapply(T &&, F &&, G &&, cute::seq<I...>) [with T=const cute::tuple<cute::_1, cute::_2, cute::C<0>> &, F=lambda [](const auto &)->auto &, G=lambda [](const auto &...)->auto, I=<0, 1, 2>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(218): here\n",
      "                  instantiation of \"auto cute::transform(const T &, F &&) [with T=cute::tuple<cute::_1, cute::_2, cute::C<0>>, F=lambda [](const auto &)->auto]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/algorithm/tuple_algorithms.hpp(263): here\n",
      "                  [ 8 instantiation contexts not shown ]\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1520): error: no instance of overloaded function \"cute::composition\" matches the argument list\n",
      "                  argument types are: (<error-type>, const cute::Layout<cute::tuple<cute::C<2>, cute::C<4>, cute::_1>, cute::tuple<cute::_1, cute::_2, cute::C<0>>>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::logical_product(const cute::Layout<LShape, LStride> &, const cute::Layout<TShape, TStride> &) [with LShape=cute::_32, LStride=cute::_1, TShape=cute::tuple<cute::C<2>, cute::C<4>, cute::_1>, TStride=cute::tuple<cute::_1, cute::_2, cute::C<0>>]\"\n",
      "      (1554): here\n",
      "                  instantiation of \"auto cute::zipped_product(const cute::Layout<LShape, LStride> &, const Tiler &) [with LShape=cute::_32, LStride=cute::_1, Tiler=cute::Layout<cute::tuple<cute::C<2>, cute::C<4>, cute::_1>, cute::tuple<cute::_1, cute::_2, cute::C<0>>>]\"\n",
      "      (1565): here\n",
      "                  instantiation of \"auto cute::tiled_product(const cute::Layout<LShape, LStride> &, const Tiler &) [with LShape=cute::_32, LStride=cute::_1, Tiler=cute::Layout<cute::tuple<cute::C<2>, cute::C<4>, cute::_1>, cute::tuple<cute::_1, cute::_2, cute::C<0>>>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/atom/mma_atom.hpp(218): here\n",
      "                  instantiation of class \"cute::TiledMMA<MMA_Atom, AtomLayoutMNK, PermutationMNK> [with MMA_Atom=cute::MMA_Atom<cute::SM80_16x8x16_F32BF16BF16F32_TN>, AtomLayoutMNK=cute::Layout<cute::tuple<cute::C<2>, cute::C<4>, cute::_1>, cute::tuple<cute::_1, cute::_2, cute::C<0>>>, PermutationMNK=cute::tuple<cute::C<32>, cute::C<64>, cute::_16>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(95): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/layout.hpp(1554): error: no instance of overloaded function \"cute::tile_unzip\" matches the argument list\n",
      "                  argument types are: (<error-type>, const cute::Layout<cute::tuple<cute::C<2>, cute::C<4>, cute::_1>, cute::tuple<cute::_1, cute::_2, cute::C<0>>>)\n",
      "                detected during:\n",
      "                  instantiation of \"auto cute::zipped_product(const cute::Layout<LShape, LStride> &, const Tiler &) [with LShape=cute::_32, LStride=cute::_1, Tiler=cute::Layout<cute::tuple<cute::C<2>, cute::C<4>, cute::_1>, cute::tuple<cute::_1, cute::_2, cute::C<0>>>]\"\n",
      "      (1565): here\n",
      "                  instantiation of \"auto cute::tiled_product(const cute::Layout<LShape, LStride> &, const Tiler &) [with LShape=cute::_32, LStride=cute::_1, Tiler=cute::Layout<cute::tuple<cute::C<2>, cute::C<4>, cute::_1>, cute::tuple<cute::_1, cute::_2, cute::C<0>>>]\"\n",
      "      C:/Users/carlo/AppData/Local/Temp/pip-install-hh2swop5/flash-attn_c824939607fa4dc5a3f574798fbaaa70/csrc/cutlass/include\\cute/atom/mma_atom.hpp(218): here\n",
      "                  instantiation of class \"cute::TiledMMA<MMA_Atom, AtomLayoutMNK, PermutationMNK> [with MMA_Atom=cute::MMA_Atom<cute::SM80_16x8x16_F32BF16BF16F32_TN>, AtomLayoutMNK=cute::Layout<cute::tuple<cute::C<2>, cute::C<4>, cute::_1>, cute::tuple<cute::_1, cute::_2, cute::C<0>>>, PermutationMNK=cute::tuple<cute::C<32>, cute::C<64>, cute::_16>]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(95): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(153): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(155): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(156): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(158): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(159): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(160): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(162): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(163): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(164): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(165): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(166): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(168): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(169): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(170): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(171): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(172): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(174): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(195): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(196): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(197): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(198): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(199): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(200): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(201): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(202): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(203): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(204): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_kernel.h(205): error: no instance of constructor \"Tensor\" matches the argument list\n",
      "                  argument types are: (<error-type>)\n",
      "                detected during:\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_1colblock<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Is_first,Is_last,Seq_parallel,Params>(const Params &, int, int, int) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Is_first=false, Is_last=false, Seq_parallel=true, Params=Flash_bwd_params]\"\n",
      "      (833): here\n",
      "                  instantiation of \"void flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap,Params>(const Params &) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true, Params=Flash_bwd_params]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(41): here\n",
      "                  instantiation of \"void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits,Is_dropout,Is_causal,Is_local,Has_alibi,Is_even_MN,Is_even_K,Is_softcap>(Flash_bwd_params) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=false, Is_causal=true, Is_local=false, Has_alibi=true, Is_even_MN=false, Is_even_K=true, Is_softcap=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(93): here\n",
      "                  instantiation of \"void run_flash_bwd_seqk_parallel<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(127): here\n",
      "                  instantiation of \"void run_flash_bwd<Kernel_traits,Is_dropout,Is_causal>(Flash_bwd_params &, cudaStream_t) [with Kernel_traits=Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, false, cutlass::bfloat16_t, Flash_kernel_traits<128, 64, 128, 8, cutlass::bfloat16_t>>, Is_dropout=true, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_launch_template.h(238): here\n",
      "                  instantiation of \"void run_mha_bwd_hdim128<T,Is_causal>(Flash_bwd_params &, cudaStream_t) [with T=cutlass::bfloat16_t, Is_causal=true]\"\n",
      "      C:\\Users\\carlo\\AppData\\Local\\Temp\\pip-install-hh2swop5\\flash-attn_c824939607fa4dc5a3f574798fbaaa70\\csrc\\flash_attn\\src\\flash_bwd_hdim128_bf16_causal_sm80.cu(9): here\n",
      "      \n",
      "      Error limit reached.\n",
      "      100 errors detected in the compilation of \"csrc/flash_attn/src/flash_bwd_hdim128_bf16_causal_sm80.cu\".\n",
      "      Compilation terminated.\n",
      "      error: command 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.8\\\\bin\\\\nvcc' failed with exit code 4294967295\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for flash-attn\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (flash-attn)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "# Instalar dependencias específicas según la versión de la GPU\n",
    "if major_version >= 8:\n",
    "    # Para GPUs Ampere, Hopper (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
    "    %pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
    "else:\n",
    "    # Para GPUs más antiguas (V100, Tesla T4, RTX 20xx)\n",
    "    %pip install --no-deps xformers trl peft accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install https://huggingface.co/datasets/DirtyBirds-Playhouse/StableDiffusion/blob/main/triton-2.1.0-cp310-cp310-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (447754190.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    git clone https://github.com/triton-lang/triton.git;\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m \u001b[38;5;66;03m# Choose any! We auto support RoPE Scaling internally!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\carlo\\Documents\\Codigo\\Scrapping corridos tumbdos\\.conda\\lib\\site-packages\\unsloth\\__init__.py:101\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPACE_AUTHOR_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPACE_REPO_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     libcuda_dirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Version(triton\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'triton'"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "# fourbit_models = [\n",
    "#     \"unsloth/mistral-7b-bnb-4bit\",\n",
    "#     \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "#     \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "#     \"unsloth/gemma-7b-bnb-4bit\",\n",
    "#     \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b\n",
    "#     \"unsloth/gemma-2b-bnb-4bit\",\n",
    "#     \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\n",
    "#     \"unsloth/llama-3-8b-bnb-4bit\", # [NEW] 15 Trillion token Llama-3\n",
    "# ] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cargar el dataset de tu computadora\n",
    "Si ya tienes un dataset en formato JSON con el significado y la letra de la canción, puedes cargarlo de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# Cargar el dataset desde un archivo local JSON\n",
    "with open('corridos_dataset.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Formatear los datos en un Dataset de Hugging Face\n",
    "def format_dataset(data):\n",
    "    formatted_data = {\n",
    "        \"instruction\": [],\n",
    "        \"input\": [],\n",
    "        \"output\": []\n",
    "    }\n",
    "    for item in data:\n",
    "        formatted_data[\"instruction\"].append(\"Completa la letra de la canción.\")\n",
    "        formatted_data[\"input\"].append(item[\"prompt\"])  # El significado va como input\n",
    "        formatted_data[\"output\"].append(item[\"completion\"])  # La letra va como output\n",
    "    return Dataset.from_dict(formatted_data)\n",
    "\n",
    "dataset = format_dataset(data)\n",
    "\n",
    "# Previsualizar una parte del dataset para asegurarse de que está correcto\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Configurar el modelo\n",
    "Aquí cargamos un modelo preentrenado (como Llama o cualquier otro que prefieras) y aplicamos LoRA para permitir el fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo preentrenado con soporte 4bit\n",
    "max_seq_length = 2048  # Longitud máxima de secuencia\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",  # Ajusta el modelo a tu preferencia\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,  # Utilizar cuantización de 4 bits\n",
    ")\n",
    "\n",
    "# Aplicar LoRA adaptadores al modelo\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # Tamaño de los adaptadores LoRA\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # Capas LoRA\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Sin dropout para eficiencia\n",
    "    bias=\"none\",  # Sin ajuste de bias\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Usar checkpointing para ahorrar memoria\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preparar los datos para el entrenamiento\n",
    "Usamos el formato del dataset y aplicamos el preprocesamiento para que esté listo para el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar el dataset para el entrenamiento\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Formato del prompt\n",
    "        text = f\"### Instruction: {instruction}\\n### Input: {input}\\n### Response: {output}\" + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Configurar el entrenamiento\n",
    "Ahora configuramos el entrenamiento utilizando SFTTrainer de la biblioteca trl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar los argumentos del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,  # Ajustar según la capacidad de tu GPU\n",
    "    gradient_accumulation_steps=4,  # Para acumular gradientes con batches pequeños\n",
    "    warmup_steps=5,  # Pasos de calentamiento\n",
    "    max_steps=1000,  # Número máximo de pasos de entrenamiento (ajusta según tu caso)\n",
    "    learning_rate=2e-4,  # Tasa de aprendizaje\n",
    "    fp16=True,  # Usa precisión de 16 bits si tu GPU lo soporta\n",
    "    logging_steps=10,  # Cada cuántos pasos hacer logging\n",
    "    output_dir=\"./model_outputs\",  # Directorio donde se guardarán los resultados\n",
    "    save_steps=200,  # Cada cuántos pasos guardar el modelo\n",
    ")\n",
    "\n",
    "# Configurar el trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",  # Campo del dataset que contiene los textos\n",
    "    max_seq_length=max_seq_length,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Iniciar el entrenamiento\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Guardar el modelo ajustado (adaptadores LoRA)\n",
    "Una vez que el entrenamiento termine, puedes guardar los adaptadores LoRA localmente:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los adaptadores LoRA entrenados localmente\n",
    "model.save_pretrained(\"lora_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Realizar inferencias con el modelo ajustado\n",
    "Después de guardar el modelo, puedes cargarlo y usarlo para realizar inferencias. Este es un ejemplo de cómo puedes generar una letra de canción basándote en el significado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo con los adaptadores LoRA\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"lora_model\",  # La carpeta donde guardaste los adaptadores\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Poner el modelo en modo de inferencia\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Prompt para generar la letra de una canción\n",
    "song_prompt_template = \"\"\"Escribe la letra de la canción 'EL LOKERÓN' de Tito Double P.\n",
    "\n",
    "[Verso 1]\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizar el input\n",
    "inputs = tokenizer([song_prompt_template], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generar la letra de la canción\n",
    "outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "\n",
    "# Decodificar la salida\n",
    "generated_lyrics = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(generated_lyrics[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
